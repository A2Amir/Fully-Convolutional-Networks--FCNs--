{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import shutil\n",
    "import zipfile\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "from distutils.version import LooseVersion\n",
    "from urllib.request import urlretrieve\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import scipy.misc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN_8():\n",
    "    def __init__(self,data_path='./data/',image_shape=(160,576),num_classes=2,epochs=10,batch_size=20,learning_rate=0.0005,save_model=True):\n",
    "        \n",
    "        self.GPU=self.check_GPU()\n",
    "        self.data_path=data_path\n",
    "        self.TF_version=self.check_version()\n",
    "        self.dataset_folders=self.downlaod_dataset(self.data_path)\n",
    "        self.weights_path=self.downlaod_pretrained_vgg(self.data_path)\n",
    "        self.num_classes=num_classes\n",
    "        self.image_shape=image_shape\n",
    "        self.epochs=epochs\n",
    "        self.batch_size=batch_size\n",
    "        self.save_model=save_model\n",
    "        self.learning_rate=learning_rate\n",
    "\n",
    "        \n",
    "        \n",
    "    def check_version(self):\n",
    "        assert LooseVersion(tf.__version__)>=LooseVersion('1.0'),'Please use Tensorflow version 1.0 or newer. You are using {}'.format(tf.__version__)\n",
    "        return tf.__version__\n",
    "        \n",
    "    def check_GPU(self):\n",
    "        if not tf.test.gpu_device_name():\n",
    "            warnings.warn('Non GPU found. Please use a GPU to train your neural network.')\n",
    "            return False\n",
    "        else:\n",
    "            print('Default GPU Devices{}'.format(tf.test.gpu_device_name()))\n",
    "            return True\n",
    "        \n",
    "    def downlaod_pretrained_vgg(self,path):\n",
    "        \"\"\"\n",
    "        Download and extract pretrained vgg model if doesnt exist\n",
    "        :param path: Directory to downlaod the model to\n",
    "        \"\"\"\n",
    "        vgg_file_name=\"vgg.zip\"\n",
    "        vgg_path=os.path.join(path,'vgg_weights')\n",
    "        vgg_files=[os.path.join(vgg_path,'variables/variables.data-00000-of-00001'),\n",
    "                  os.path.join(vgg_path,'variables/variables.index'),\n",
    "                  os.path.join(vgg_path,'saved_model.pb')]\n",
    "        missing_files=[vgg_file for vgg_file in vgg_files if not os.path.exists(vgg_file)]\n",
    "        #print(missing_files)\n",
    "        if missing_files:\n",
    "            #Clean vgg directory\n",
    "            if os.path.exists(vgg_path):\n",
    "                #clean vgg path\n",
    "                shutil.rmtree(vgg_path)\n",
    "            os.mkdir(vgg_path)\n",
    "            \n",
    "            print('Downloading pretrained vgg nodel...')\n",
    "            with DLProgress(unit='B',unit_scale=True,miniters=1) as pbar:\n",
    "                urlretrieve('https://s3-us-west-1.amazonaws.com/udacity-selfdrivingcar/vgg.zip',\n",
    "                           os.path.join(path,vgg_file_name),\n",
    "                           pbar.hook)\n",
    "        \n",
    "            #Extract vgg file\n",
    "            zip_ref=zipfile.ZipFile(os.path.join(path,vgg_file_name),'r')\n",
    "            zip_ref.extractall(vgg_path)\n",
    "            zip_ref.close()\n",
    "            os.remove(os.path.join(path, vgg_filename))\n",
    "        else:\n",
    "            print('Vgg weights are available')\n",
    "        return vgg_path         \n",
    "                                \n",
    "                                \n",
    "    def load_vgg(self,sess):\n",
    "        \"\"\"\n",
    "        Load Pretrained VGG Model into TensorFlow\n",
    "        :param sess: TensorFlow Session\n",
    "        :param weights_path: the path to the vgg folder where contains weights variables\n",
    "        :return: Tuple of Tensors from VGG Model(Input Image,Keep_probab,layer3_out,layer4_out,layer7_out)\n",
    "        \"\"\"\n",
    "        vgg_name='vgg16'\n",
    "        vgg_input_tensor_name='image_input:0'\n",
    "        vgg_keep_prob_tensor_name='keep_prob:0'\n",
    "        vgg_layer3_out_tensor_name='layer3_out:0'\n",
    "        vgg_layer4_out_tensor_name='layer4_out:0'\n",
    "        vgg_layer7_out_tensor_name='layer7_out:0'\n",
    "        \n",
    "        tf.saved_model.loader.load(sess,[vgg_name],self.weights_path)\n",
    "        graph=tf.get_default_graph()\n",
    "        image_input=graph.get_tensor_by_name(vgg_input_tensor_name)\n",
    "        keep_prob=graph.get_tensor_by_name(vgg_keep_prob_tensor_name)\n",
    "        layer3_out=graph.get_tensor_by_name(vgg_layer3_out_tensor_name)\n",
    "        layer4_out=graph.get_tensor_by_name(vgg_layer4_out_tensor_name)\n",
    "        layer7_out=graph.get_tensor_by_name(vgg_layer7_out_tensor_name)\n",
    "\n",
    "        return image_input,keep_prob,layer3_out,layer4_out,layer7_out\n",
    "    \n",
    "    def encoder(self,layer7_out,num_classes):\n",
    "        \"\"\"\n",
    "        Create the encoder portion of the FCN \n",
    "        :param layer7_out:TF Tensor for VGG Layer 7 output\n",
    "        :return: tensor for the last layer of the encoder which ic 1 by 1 convolution \n",
    "        \"\"\"\n",
    "        #1*1 conv\n",
    "        conv11_out=tf.layers.conv2d(layer7_out,num_classes,1,\n",
    "                                padding='same',\n",
    "                                kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3),\n",
    "                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        return conv11_out\n",
    "    \n",
    "    def decoder(self,conv11_out,layer3_out,layer4_out,num_classes):\n",
    "        \n",
    "        \"\"\"\n",
    "        Create the Eecoder portion of the FCN \n",
    "        :param conv11_out:TF Tensor for last layer of the encoder \n",
    "        :param layer3_out:TF Tensor for VGG Layer 3 output\n",
    "        :param layer4_out:TF Tensor for VGG Layer 4 output\n",
    "        :return: tensor for the last layer of the decoder.\n",
    "        \"\"\"\n",
    "        \n",
    "        #upsample\n",
    "        l8_out=tf.layers.conv2d_transpose(conv11_out,num_classes,4,2,\n",
    "                                         padding='same',\n",
    "                                         kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3),\n",
    "                                         kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        # check the shapes\n",
    "        l4=tf.layers.conv2d(layer4_out,num_classes,1,padding='same',\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3),\n",
    "                             kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        # skip connection (element-wise addition)\n",
    "        l9_in=tf.add(l8_out,l4)\n",
    "        \n",
    "        #upsample by 2\n",
    "        l9_out=tf.layers.conv2d_transpose(l9_in,num_classes,4,2,\n",
    "                                         padding='same',\n",
    "                                         kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3),\n",
    "                                         kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        \n",
    "        l3=tf.layers.conv2d(layer3_out,num_classes,1,padding='same',\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3),\n",
    "                             kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        # skip connection (element-wise addition)\n",
    "        l10_in=tf.add(l9_out,l3)\n",
    "        \n",
    "        #upsample\n",
    "        ll0_out = tf.layers.conv2d_transpose(l10_in, num_classes, 16, 8,\n",
    "                                    padding='same',\n",
    "                                    kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3),\n",
    "                                    kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        \n",
    "        return ll0_out\n",
    "    \n",
    "    def downlaod_dataset(self,path):\n",
    "        \"\"\"\n",
    "        To load data from the the data directory\n",
    "        :param data_folder: path to the folder contains all the dataset\n",
    "        :return :lists of images and ground truths path\n",
    "        \"\"\"\n",
    "        images_path=[]\n",
    "        gt_images_path=[]\n",
    "\n",
    "\n",
    "        dataset_file_name=\"data_road.zip\"\n",
    "        dataset_path=os.path.join(path,'data_road')\n",
    "        dataset_folders=[os.path.join(path,'data_road/data_road/training/image_2/'),\n",
    "                  os.path.join(path,'data_road/data_road/training/gt_image_2/'),\n",
    "                  os.path.join(path,'data_road/data_road/testing/image_2/')]\n",
    "        \n",
    "        missing_folder=[dataset_folder for dataset_folder in dataset_folders if not os.path.exists(dataset_folder)]\n",
    "        #print(missing_folder)\n",
    "        if missing_folder:\n",
    "            #Clean vgg directory\n",
    "            if os.path.exists(dataset_path):\n",
    "                #clean vgg path\n",
    "                shutil.rmtree(dataset_path)\n",
    "            os.mkdir(dataset_path)\n",
    "            \n",
    "            print('Downloading dataset...')\n",
    "            with DLProgress(unit='B',unit_scale=True,miniters=1) as pbar:\n",
    "                urlretrieve('https://s3.eu-central-1.amazonaws.com/avg-kitti/data_road.zip',\n",
    "                           os.path.join(path,dataset_file_name),\n",
    "                           pbar.hook)\n",
    "        \n",
    "            #Extract vgg file\n",
    "            zip_ref=zipfile.ZipFile(os.path.join(path,dataset_file_name),'r')\n",
    "            zip_ref.extractall(dataset_path)\n",
    "            zip_ref.close()\n",
    "            os.remove(os.path.join(path, dataset_file_name))\n",
    "        else:\n",
    "            print('Dataset is available')\n",
    "            \n",
    "          \n",
    "        #gt_images_path={re.sub(r'_(road|lane)_','_',os.path.basename(path)):path for path in }\n",
    "        return dataset_folders\n",
    "    \n",
    "    def generator(self,batch_size,image_shape):\n",
    "        \"\"\"\n",
    "        create batches of training data\n",
    "        :param data_folder: Path to folder that contains all the datasets\n",
    "        :param image_shape: Tuple - Shape of image\n",
    "        :param batch_size: Batch Size\n",
    "        :return: Batches of training data\n",
    "        \"\"\"\n",
    "        train_imgs=glob.glob(self.dataset_folders[0]+ '*.png')\n",
    "        train_gts=glob.glob(self.dataset_folders[1]+ '*_road_*.png')\n",
    "        train_imgs=sorted(train_imgs)\n",
    "        train_gts=sorted(train_gts)\n",
    "        \n",
    "        background_color = np.array([255, 0, 0])\n",
    "        \n",
    "        #Shuffle two list at once with same order\n",
    "        c=list(zip(train_imgs,train_gts))\n",
    "        random.shuffle(c)\n",
    "        train_imgs,train_gts=zip(*c)\n",
    "        \n",
    "        for batch in range(0,len(train_imgs),batch_size):\n",
    "            X_train=[]\n",
    "            y_train=[]\n",
    "            \n",
    "            train_batches=train_imgs[batch:batch+batch_size]\n",
    "            train_Gtbatches=train_gts[batch:batch+batch_size]\n",
    "            \n",
    "            for image_file,gt_image_file in zip(train_batches,train_Gtbatches):\n",
    "                \n",
    "                image = cv2.resize(cv2.cvtColor(cv2.imread(image_file),cv2.COLOR_BGR2RGB), image_shape)\n",
    "                gt_image =cv2.resize(cv2.cvtColor(cv2.imread(gt_image_file),cv2.COLOR_BGR2RGB), image_shape)\n",
    "\n",
    "                gt_bg = np.all(gt_image == background_color, axis=2)\n",
    "\n",
    "                gt_bg = gt_bg.reshape(*gt_bg.shape, 1)\n",
    "                gt_image = np.concatenate((gt_bg, np.invert(gt_bg)), axis=2)\n",
    "                \n",
    "                X_train.append(image)\n",
    "                y_train.append(gt_image)\n",
    "\n",
    "            yield np.array(X_train), np.array(y_train)\n",
    "                \n",
    "\n",
    "    def optimize(self,nn_last_layer,correct_label,learning_rate,num_classes):\n",
    "        \n",
    "        \"\"\"\n",
    "        Build the TensorFLow loss and optimizer operations.\n",
    "        :param nn_last_layer: TF Tensor of the last layer in the neural network\n",
    "        :param correct_label: TF Placeholder for the correct label image\n",
    "        :param learning_rate: TF Placeholder for the learning rate\n",
    "        :param num_classes: Number of classes to classify\n",
    "        :return: Tuple of (logits, train_op, cross_entropy_loss)\n",
    "        \"\"\"\n",
    "        logits=tf.reshape(nn_last_layer,(-1,num_classes))\n",
    "        fix_label=tf.reshape(correct_label,(-1,num_classes))\n",
    "        cross_entropy_loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=fix_label,logits=logits))\n",
    "        optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        train_op=optimizer.minimize(cross_entropy_loss)\n",
    "        return logits,train_op,cross_entropy_loss\n",
    "    \n",
    "        \n",
    "    def train_nn(self,sess,epochs,batch_size,train_op,cross_entropy_loss,\n",
    "                    image_input,image_shape,correct_label,keep_prob,learning_rate):\n",
    "         \n",
    "        \"\"\"\n",
    "        Train neural network and print out the loss during training.\n",
    "        :param sess: TF Session\n",
    "        :param epochs: Number of epochs\n",
    "        :param batch_size: Batch size\n",
    "        :param get_batches_fn: Function to get batches of training data.  Call using get_batches_fn(batch_size)\n",
    "        :param train_op: TF Operation to train the neural network\n",
    "        :param cross_entropy_loss: TF Tensor for the amount of loss\n",
    "        :param input_image: TF Placeholder for input images\n",
    "        :param correct_label: TF Placeholder for label images\n",
    "        :param keep_prob: TF Placeholder for dropout keep probability\n",
    "        :param learning_rate: TF Placeholder for learning rate\n",
    "        \"\"\"\n",
    "        saver = tf.train.Saver()\n",
    "        model_path=os.path.join(self.data_path,'fcn_model/FCN.ckpt')        #to save model\n",
    "        if not os.path.exists(os.path.dirname(model_path)):\n",
    "            os.mkdir(os.path.dirname(model_path))\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in tqdm(range(epochs)):\n",
    "            print(\"Epoch >>>{}...\".format(i+1))\n",
    "            for image,label in self.generator(batch_size,image_shape):\n",
    "                \n",
    "                _,loss=sess.run([train_op,cross_entropy_loss],feed_dict={image_input:image,correct_label:label,keep_prob: 0.5,\n",
    "                                          learning_rate: self.learning_rate})\n",
    "                print(\"LOSS >>>> {:.3f}\".format(loss))\n",
    "                if self.save_model:\n",
    "                     saver.save(sess, model_path)\n",
    "                \n",
    "        print('Training Finished. Saved the weights to: {}'.format(model_path))  \n",
    "\n",
    "    def gen_test_output(self,sess, logits, keep_prob, image_pl, image_shape):\n",
    "        \"\"\"\n",
    "        Generate test output using the test images and save them\n",
    "        :param sess: TF session\n",
    "        :param logits: TF Tensor for the logits\n",
    "        :param keep_prob: TF Placeholder for the dropout keep robability\n",
    "        :param image_pl: TF Placeholder for the image placeholder\n",
    "        :param image_shape: Tuple - Shape of image\n",
    "        \"\"\"\n",
    "        imgs=glob.glob(self.dataset_folders[2]+ '*.png')\n",
    "        output_dir=os.path.join(self.dataset_folders[2], 'result')\n",
    "        \n",
    "        for img in imgs:\n",
    "            image = cv2.resize(cv2.cvtColor(cv2.imread(img),cv2.COLOR_BGR2RGB), image_shape)\n",
    "\n",
    "            im_softmax = sess.run([tf.nn.softmax(logits)],{keep_prob: 1.0, image_pl: [image]})\n",
    "            print(im_softmax)\n",
    "            print()\n",
    "            print(im_softmax[0][:, 1])\n",
    "\n",
    "            im_softmax = im_softmax[0][:, 1].reshape(image_shape[0], image_shape[1])\n",
    "            segmentation = (im_softmax > 0.5).reshape(image_shape[0], image_shape[1], 1)\n",
    "            mask = np.dot(segmentation, np.array([[0, 255, 0, 127]]))\n",
    "            mask = scipy.misc.toimage(mask, mode=\"RGBA\")\n",
    "            im = Image.fromarray(A)\n",
    "            street_im = scipy.misc.toimage(image)\n",
    "            street_im.paste(mask, box=None, mask=mask)\n",
    "            plt.imsave(output_dir, image)\n",
    "\n",
    "        \n",
    "        \n",
    "    def run_train(self):\n",
    "            \n",
    "        with tf.Session() as sess:\n",
    "            correct_label=tf.placeholder(tf.int32,[None,None,None,self.num_classes],name='correct_label')\n",
    "            learning_rate=tf.placeholder(tf.float32,name='learning_rate')\n",
    "                \n",
    "            image_input,keep_prob,layer3_out,layer4_out,layer7_out=self.load_vgg(sess)\n",
    "            encoder=self.encoder(layer7_out,self.num_classes)\n",
    "            decoder_last_layer=self.decoder(encoder,layer3_out,layer4_out,self.num_classes)\n",
    "            logits,train_op,cross_entropy_loss=self.optimize(decoder_last_layer,correct_label,learning_rate,self.num_classes)\n",
    "            self.train_nn(sess,self.epochs,self.batch_size,train_op,cross_entropy_loss,\n",
    "                    image_input,self.image_shape,correct_label,keep_prob,learning_rate)  \n",
    "            self.gen_test_output(sess, logits, keep_prob, image_input, self.image_shape)    \n",
    "        \n",
    "class DLProgress(tqdm):\n",
    "    last_bloack=0\n",
    "\n",
    "    def hook(self,block_num=1,block_size=1,total_size=None):\n",
    "        self.total=total_size\n",
    "        self.update((block_num-self.last_bloack)*block_size)\n",
    "        self.last_bloack=block_num   \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is available\n",
      "Vgg weights are available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Non GPU found. Please use a GPU to train your neural network.\n"
     ]
    }
   ],
   "source": [
    "fcn=FCN_8(epochs=1,batch_size=40)\n",
    "#print(fcn.dataset_folders)\n",
    "#print(fcn.weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-c2b2be47a66f>:79: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./data/vgg_weights\\variables\\variables\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-3-c2b2be47a66f>:99: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From <ipython-input-3-c2b2be47a66f>:116: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2DTranspose` instead.\n",
      "WARNING:tensorflow:From <ipython-input-3-c2b2be47a66f>:242: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch >>>1...\n",
      "LOSS >>>> 1.775\n",
      "LOSS >>>> 5.455\n",
      "LOSS >>>> 1.374\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Failed to rename: ./data/fcn_model/FCN.ckpt.data-00000-of-00001.tempstate8330134777797624001 to: ./data/fcn_model/FCN.ckpt.data-00000-of-00001 : Der Prozess kann nicht auf die Datei zugreifen, da sie von einem anderen Prozess verwendet wird.\r\n; Broken pipe\n\t [[node save/SaveV2_1 (defined at <ipython-input-3-c2b2be47a66f>:264) ]]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node save/SaveV2_1:\n conv2_1/biases (defined at <ipython-input-3-c2b2be47a66f>:79)\t\n conv2d_2/bias (defined at <ipython-input-3-c2b2be47a66f>:132)\t\n conv2d_transpose_2/kernel/Adam_1 (defined at <ipython-input-3-c2b2be47a66f>:244)\t\n conv2d_transpose/bias (defined at <ipython-input-3-c2b2be47a66f>:116)\t\n conv2d/bias (defined at <ipython-input-3-c2b2be47a66f>:99)\t\n conv2d_transpose_2/bias (defined at <ipython-input-3-c2b2be47a66f>:140)\t\n conv2d_transpose_1/kernel (defined at <ipython-input-3-c2b2be47a66f>:128)\t\n conv2d_1/kernel (defined at <ipython-input-3-c2b2be47a66f>:120)\n\nOriginal stack trace for 'save/SaveV2_1':\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n    app.start()\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 563, in start\n    self.io_loop.start()\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\asyncio\\base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\asyncio\\base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n    self.run()\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 361, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 541, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3242, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3319, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-87dac42ffede>\", line 1, in <module>\n    fcn.run_train()\n  File \"<ipython-input-3-c2b2be47a66f>\", line 324, in run_train\n    image_input,self.image_shape,correct_label,keep_prob,learning_rate)\n  File \"<ipython-input-3-c2b2be47a66f>\", line 264, in train_nn\n    saver = tf.train.Saver()\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 825, in __init__\n    self.build()\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 837, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 875, in _build\n    build_restore=build_restore)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 505, in _build_internal\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 206, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 122, in save_op\n    tensors)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1945, in save_v2\n    name=name)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1341\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Failed to rename: ./data/fcn_model/FCN.ckpt.data-00000-of-00001.tempstate8330134777797624001 to: ./data/fcn_model/FCN.ckpt.data-00000-of-00001 : Der Prozess kann nicht auf die Datei zugreifen, da sie von einem anderen Prozess verwendet wird.\r\n; Broken pipe\n\t [[{{node save/SaveV2_1}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-87dac42ffede>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfcn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-c2b2be47a66f>\u001b[0m in \u001b[0;36mrun_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_last_layer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcorrect_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m             self.train_nn(sess,self.epochs,self.batch_size,train_op,cross_entropy_loss,\n\u001b[1;32m--> 324\u001b[1;33m                     image_input,self.image_shape,correct_label,keep_prob,learning_rate)  \n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen_test_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-c2b2be47a66f>\u001b[0m in \u001b[0;36mtrain_nn\u001b[1;34m(self, sess, epochs, batch_size, train_op, cross_entropy_loss, image_input, image_shape, correct_label, keep_prob, learning_rate)\u001b[0m\n\u001b[0;32m    276\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LOSS >>>> {:.3f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m                      \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training Finished. Saved the weights to: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs, save_debug_info)\u001b[0m\n\u001b[0;32m   1188\u001b[0m               \"Parent directory of {} doesn't exist, can't save.\".format(\n\u001b[0;32m   1189\u001b[0m                   save_path))\n\u001b[1;32m-> 1190\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwrite_meta_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs, save_debug_info)\u001b[0m\n\u001b[0;32m   1171\u001b[0m           model_checkpoint_path = sess.run(\n\u001b[0;32m   1172\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1173\u001b[1;33m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[0;32m   1174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1368\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1370\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Failed to rename: ./data/fcn_model/FCN.ckpt.data-00000-of-00001.tempstate8330134777797624001 to: ./data/fcn_model/FCN.ckpt.data-00000-of-00001 : Der Prozess kann nicht auf die Datei zugreifen, da sie von einem anderen Prozess verwendet wird.\r\n; Broken pipe\n\t [[node save/SaveV2_1 (defined at <ipython-input-3-c2b2be47a66f>:264) ]]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node save/SaveV2_1:\n conv2_1/biases (defined at <ipython-input-3-c2b2be47a66f>:79)\t\n conv2d_2/bias (defined at <ipython-input-3-c2b2be47a66f>:132)\t\n conv2d_transpose_2/kernel/Adam_1 (defined at <ipython-input-3-c2b2be47a66f>:244)\t\n conv2d_transpose/bias (defined at <ipython-input-3-c2b2be47a66f>:116)\t\n conv2d/bias (defined at <ipython-input-3-c2b2be47a66f>:99)\t\n conv2d_transpose_2/bias (defined at <ipython-input-3-c2b2be47a66f>:140)\t\n conv2d_transpose_1/kernel (defined at <ipython-input-3-c2b2be47a66f>:128)\t\n conv2d_1/kernel (defined at <ipython-input-3-c2b2be47a66f>:120)\n\nOriginal stack trace for 'save/SaveV2_1':\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n    app.start()\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 563, in start\n    self.io_loop.start()\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\asyncio\\base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\asyncio\\base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n    self.run()\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 361, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 541, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3242, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3319, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-87dac42ffede>\", line 1, in <module>\n    fcn.run_train()\n  File \"<ipython-input-3-c2b2be47a66f>\", line 324, in run_train\n    image_input,self.image_shape,correct_label,keep_prob,learning_rate)\n  File \"<ipython-input-3-c2b2be47a66f>\", line 264, in train_nn\n    saver = tf.train.Saver()\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 825, in __init__\n    self.build()\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 837, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 875, in _build\n    build_restore=build_restore)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 505, in _build_internal\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 206, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 122, in save_op\n    tensors)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1945, in save_v2\n    name=name)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\ziaeeamir\\AppData\\Local\\Continuum\\anaconda3\\envs\\Huber\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "fcn.run_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
